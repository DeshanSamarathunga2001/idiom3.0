{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation for Idiom Translation\n",
    "\n",
    "This notebook creates augmented training examples from the base dataset to improve model generalization.\n",
    "\n",
    "## Augmentation Strategy:\n",
    "1. **Tagged idiomatic** - Original examples with `<IDIOM>` tags (already done)\n",
    "2. **Untagged versions** - Same examples without tags (for robustness)\n",
    "3. **Quality validation** - Ensure no data corruption\n",
    "\n",
    "**Note**: We use conservative augmentation to maintain quality. No fake data is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.augmentation import (\n",
    "    create_augmented_examples,\n",
    "    validate_augmentation,\n",
    "    save_augmented_data,\n",
    "    augment_dataset\n",
    ")\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_path = '../data/processed/train.json'\n",
    "\n",
    "with open(train_path, 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(train_data)} training examples\")\n",
    "print(\"\\nSample example:\")\n",
    "print(json.dumps(train_data[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Augmented Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate augmented examples\n",
    "augmented_data = create_augmented_examples(train_data)\n",
    "\n",
    "print(f\"\\nAugmented dataset size: {len(augmented_data)}\")\n",
    "print(f\"Augmentation ratio: {len(augmented_data) / len(train_data):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compare Original and Augmented Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of augmentation\n",
    "print(\"=== Augmentation Examples ===\")\n",
    "\n",
    "for i in range(min(3, len(train_data))):\n",
    "    original = train_data[i]\n",
    "    # Find the corresponding augmented versions\n",
    "    augmented_versions = [\n",
    "        ex for ex in augmented_data \n",
    "        if ex['idiom_en'] == original['idiom_en'] and ex['target_si'] == original['target_si']\n",
    "    ][:2]  # Get first 2 variants\n",
    "    \n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"Idiom: {original['idiom_en']}\")\n",
    "    \n",
    "    for j, variant in enumerate(augmented_versions):\n",
    "        aug_type = variant.get('augmentation_type', 'original')\n",
    "        print(f\"\\nVariant {j+1} ({aug_type}):\")\n",
    "        print(f\"  Source: {variant['source_en'][:80]}...\")\n",
    "        print(f\"  Target: {variant['target_si'][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Validate Augmentation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate augmentation\n",
    "validation_stats = validate_augmentation(train_data, augmented_data)\n",
    "\n",
    "print(\"\\n=== Validation Results ===\")\n",
    "print(f\"Original count: {validation_stats['original_count']}\")\n",
    "print(f\"Augmented count: {validation_stats['augmented_count']}\")\n",
    "print(f\"Augmentation ratio: {validation_stats['augmentation_ratio']:.2f}x\")\n",
    "\n",
    "if validation_stats['issues']:\n",
    "    print(f\"\\n⚠️ Found {len(validation_stats['issues'])} issues:\")\n",
    "    for issue in validation_stats['issues'][:5]:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"\\n✅ No issues found - augmentation is valid!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save augmented data\n",
    "output_path = '../data/processed/augmented_train.json'\n",
    "save_augmented_data(augmented_data, output_path)\n",
    "\n",
    "print(f\"\\n✓ Augmented dataset saved to {output_path}\")\n",
    "print(f\"  Total examples: {len(augmented_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Augmentation Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dataset size comparison\n",
    "sizes = pd.DataFrame({\n",
    "    'Dataset': ['Original', 'Augmented'],\n",
    "    'Examples': [len(train_data), len(augmented_data)]\n",
    "})\n",
    "\n",
    "axes[0].bar(sizes['Dataset'], sizes['Examples'], color=['#3498db', '#2ecc71'])\n",
    "axes[0].set_title('Dataset Size: Original vs Augmented', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Examples')\n",
    "for i, v in enumerate(sizes['Examples']):\n",
    "    axes[0].text(i, v + 10, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Augmentation type breakdown\n",
    "aug_types = pd.Series([ex.get('augmentation_type', 'original') for ex in augmented_data]).value_counts()\n",
    "axes[1].pie(aug_types.values, labels=aug_types.index, autopct='%1.1f%%',\n",
    "            colors=['#3498db', '#e74c3c', '#f39c12'], startangle=90)\n",
    "axes[1].set_title('Augmentation Type Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/augmentation_statistics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved to outputs/augmentation_statistics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary\n",
    "\n",
    "Data augmentation completed successfully!\n",
    "\n",
    "- ✅ Created augmented training examples\n",
    "- ✅ Validated augmentation quality\n",
    "- ✅ Saved to `data/processed/augmented_train.json`\n",
    "- ✅ Maintained data quality (no fake idioms)\n",
    "\n",
    "**Statistics**:\n",
    "- Original examples: (calculated during execution)\n",
    "- Augmented examples: {len(augmented_data)}\n",
    "- Augmentation ratio: {len(augmented_data) / len(train_data):.2f}x\n",
    "\n",
    "**Next Step**: Run `03_model_training.ipynb` to fine-tune the NLLB model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}