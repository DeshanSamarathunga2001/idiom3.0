{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for Idiom-Aware Translation\n",
    "\n",
    "This notebook processes the raw Excel dataset containing English-Sinhala idiom pairs and prepares it for model training.\n",
    "\n",
    "## Steps:\n",
    "1. Load the Excel dataset\n",
    "2. Validate data quality\n",
    "3. Split into train/test sets (first 50 rows for testing)\n",
    "4. Tag idioms with `<IDIOM>` markers\n",
    "5. Export to JSON format\n",
    "6. Display statistics and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.data_processor import (\n",
    "    load_excel,\n",
    "    validate_data,\n",
    "    split_data,\n",
    "    convert_to_json_format,\n",
    "    export_to_json,\n",
    "    process_dataset\n",
    ")\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file\n",
    "excel_path = '../data/raw/idiom_dataset.xlsx'\n",
    "df = load_excel(excel_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Show first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality\n",
    "stats = validate_data(df)\n",
    "\n",
    "print(\"\\n=== Data Quality Report ===\")\n",
    "print(f\"Total rows: {stats['total_rows']}\")\n",
    "print(f\"\\nMissing values per column:\")\n",
    "for col, count in stats['missing_values'].items():\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count}\")\n",
    "\n",
    "if stats['evaluation_counts']:\n",
    "    print(f\"\\nEvaluation status:\")\n",
    "    for status, count in stats['evaluation_counts'].items():\n",
    "        print(f\"  {status}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Process and Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the full dataset\n",
    "output_dir = '../data/processed'\n",
    "test_size = 50\n",
    "\n",
    "processing_stats = process_dataset(\n",
    "    excel_path=excel_path,\n",
    "    output_dir=output_dir,\n",
    "    test_size=test_size\n",
    ")\n",
    "\n",
    "print(\"\\n=== Processing Complete ===\")\n",
    "print(f\"Training examples: {processing_stats['train_examples']}\")\n",
    "print(f\"Test examples: {processing_stats['test_examples']}\")\n",
    "print(f\"\\nOutput files:\")\n",
    "for key, path in processing_stats['output_files'].items():\n",
    "    print(f\"  {key}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inspect Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display sample processed data\n",
    "with open('../data/processed/test.json', 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(\"Sample test example:\")\n",
    "print(json.dumps(test_data[0], indent=2, ensure_ascii=False))\n",
    "\n",
    "# Verify idiom tagging\n",
    "print(\"\\n=== Idiom Tagging Examples ===\")\n",
    "for i, example in enumerate(test_data[:3]):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Idiom: {example['idiom_en']}\")\n",
    "    print(f\"Source: {example['source_en'][:100]}...\")\n",
    "    print(f\"Tagged: {'<IDIOM>' in example['source_en']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistics and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data for analysis\n",
    "with open('../data/processed/train.json', 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Calculate statistics\n",
    "total_idioms = len(set([ex['idiom_en'] for ex in train_data + test_data]))\n",
    "validation_yes = len([ex for ex in train_data + test_data if ex['evaluation'] == 'Yes'])\n",
    "\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"Total unique idioms: {total_idioms}\")\n",
    "print(f\"Training examples: {len(train_data)}\")\n",
    "print(f\"Test examples: {len(test_data)}\")\n",
    "print(f\"Validated examples: {validation_yes}\")\n",
    "print(f\"Validation rate: {validation_yes / (len(train_data) + len(test_data)) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize data distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Dataset split\n",
    "split_data_viz = pd.DataFrame({\n",
    "    'Split': ['Train', 'Test'],\n",
    "    'Count': [len(train_data), len(test_data)]\n",
    "})\n",
    "axes[0].bar(split_data_viz['Split'], split_data_viz['Count'], color=['#3498db', '#e74c3c'])\n",
    "axes[0].set_title('Train/Test Split', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Examples')\n",
    "for i, v in enumerate(split_data_viz['Count']):\n",
    "    axes[0].text(i, v + 5, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Evaluation status\n",
    "eval_counts = pd.Series([ex['evaluation'] for ex in train_data + test_data]).value_counts()\n",
    "axes[1].pie(eval_counts.values, labels=eval_counts.index, autopct='%1.1f%%', \n",
    "            colors=['#2ecc71', '#e67e22'], startangle=90)\n",
    "axes[1].set_title('Evaluation Status', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/data_statistics.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved to outputs/data_statistics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sentence lengths\n",
    "train_lengths = [len(ex['source_en'].split()) for ex in train_data]\n",
    "test_lengths = [len(ex['source_en'].split()) for ex in test_data]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_lengths, bins=20, alpha=0.7, color='#3498db', edgecolor='black')\n",
    "plt.title('Training Set - Sentence Length Distribution', fontweight='bold')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(sum(train_lengths)/len(train_lengths), color='red', linestyle='--', \n",
    "            label=f'Mean: {sum(train_lengths)/len(train_lengths):.1f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(test_lengths, bins=20, alpha=0.7, color='#e74c3c', edgecolor='black')\n",
    "plt.title('Test Set - Sentence Length Distribution', fontweight='bold')\n",
    "plt.xlabel('Number of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(sum(test_lengths)/len(test_lengths), color='red', linestyle='--',\n",
    "            label=f'Mean: {sum(test_lengths)/len(test_lengths):.1f}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/sentence_length_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Visualization saved to outputs/sentence_length_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Data preparation completed successfully! The dataset has been:\n",
    "- ✅ Loaded and validated\n",
    "- ✅ Split into train and test sets\n",
    "- ✅ Idioms tagged with `<IDIOM>` markers\n",
    "- ✅ Exported to JSON format\n",
    "- ✅ Statistics calculated and visualized\n",
    "\n",
    "**Next Step**: Run `02_data_augmentation.ipynb` to create augmented training examples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
