{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Testing\n",
    "\n",
    "This notebook tests the fine-tuned NLLB model on the 50 test examples.\n",
    "\n",
    "## Testing Pipeline:\n",
    "1. Load the fine-tuned model\n",
    "2. Load test dataset\n",
    "3. Generate translations for all test examples\n",
    "4. Display side-by-side comparisons\n",
    "5. Save predictions for evaluation\n",
    "6. Quick quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.inference import (\n",
    "    load_trained_model,\n",
    "    translate,\n",
    "    batch_translate,\n",
    "    translate_with_idiom\n",
    ")\n",
    "from src.trainer import load_config\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Configuration and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('../config/training_config.yaml')\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model_path = config['paths']['final_model']\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "model, tokenizer = load_trained_model(\n",
    "    checkpoint_path=model_path,\n",
    "    base_model=config['model']['base_model']\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_path = config['data']['test_json']\n",
    "\n",
    "with open(test_path, 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(test_data)} test examples\")\n",
    "print(\"\\nSample test example:\")\n",
    "print(json.dumps(test_data[0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract source texts\n",
    "source_texts = [example['source_en'] for example in test_data]\n",
    "\n",
    "# Generate translations in batches\n",
    "print(\"Generating translations...\")\n",
    "predictions = batch_translate(\n",
    "    texts=source_texts,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    src_lang=config['model']['source_lang'],\n",
    "    tgt_lang=config['model']['target_lang'],\n",
    "    max_length=config['training']['max_length'],\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(predictions)} translations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Display Side-by-Side Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comparisons for first 10 examples\n",
    "print(\"=\" * 80)\n",
    "print(\"TRANSLATION COMPARISONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(min(10, len(test_data))):\n",
    "    example = test_data[i]\n",
    "    prediction = predictions[i]\n",
    "    \n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"English Idiom: {example['idiom_en']}\")\n",
    "    print(f\"Sinhala Idiom: {example['idiom_si']}\")\n",
    "    print(f\"\\nSource (EN): {example['source_en']}\")\n",
    "    print(f\"\\nExpected (SI): {example['target_si']}\")\n",
    "    print(f\"\\nPredicted (SI): {prediction}\")\n",
    "    \n",
    "    # Quick check if Sinhala idiom is present\n",
    "    idiom_present = example['idiom_si'] in prediction\n",
    "    print(f\"\\nIdiom Correct: {'✅ Yes' if idiom_present else '❌ No'}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Results DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for easy viewing\n",
    "results_df = pd.DataFrame({\n",
    "    'Idiom_EN': [ex['idiom_en'] for ex in test_data],\n",
    "    'Idiom_SI': [ex['idiom_si'] for ex in test_data],\n",
    "    'Source': [ex['source_en'] for ex in test_data],\n",
    "    'Reference': [ex['target_si'] for ex in test_data],\n",
    "    'Prediction': predictions,\n",
    "    'Idiom_Found': [ex['idiom_si'] in pred for ex, pred in zip(test_data, predictions)]\n",
    "})\n",
    "\n",
    "# Display summary\n",
    "print(\"\\n=== Quick Summary ===\")\n",
    "print(f\"Total examples: {len(results_df)}\")\n",
    "print(f\"Idiom found in translation: {results_df['Idiom_Found'].sum()} / {len(results_df)}\")\n",
    "print(f\"Idiom accuracy: {results_df['Idiom_Found'].sum() / len(results_df) * 100:.1f}%\")\n",
    "\n",
    "# Show first few rows\n",
    "print(\"\\nFirst 5 results:\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for saving\n",
    "results = []\n",
    "for i, (example, prediction) in enumerate(zip(test_data, predictions)):\n",
    "    result = {\n",
    "        'example_id': i,\n",
    "        'idiom_en': example['idiom_en'],\n",
    "        'idiom_si': example['idiom_si'],\n",
    "        'source_en': example['source_en'],\n",
    "        'reference_si': example['target_si'],\n",
    "        'prediction_si': prediction,\n",
    "        'idiom_present': example['idiom_si'] in prediction\n",
    "    }\n",
    "    results.append(result)\n",
    "\n",
    "# Save to JSON\n",
    "output_path = Path(config['outputs']['predictions'])\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(f\"  Total predictions: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Translations Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some successful and unsuccessful translations\n",
    "successful = results_df[results_df['Idiom_Found'] == True].head(3)\n",
    "unsuccessful = results_df[results_df['Idiom_Found'] == False].head(3)\n",
    "\n",
    "print(\"=== Successful Translations (Idiom Found) ===\")\n",
    "for idx, row in successful.iterrows():\n",
    "    print(f\"\\nIdiom: {row['Idiom_EN']} → {row['Idiom_SI']}\")\n",
    "    print(f\"Prediction: {row['Prediction'][:100]}...\")\n",
    "\n",
    "print(\"\\n\\n=== Unsuccessful Translations (Idiom Not Found) ===\")\n",
    "for idx, row in unsuccessful.iterrows():\n",
    "    print(f\"\\nIdiom: {row['Idiom_EN']} → {row['Idiom_SI']}\")\n",
    "    print(f\"Expected: {row['Reference'][:100]}...\")\n",
    "    print(f\"Got: {row['Prediction'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Inference and testing completed!\n",
    "\n",
    "- ✅ Fine-tuned model loaded\n",
    "- ✅ 50 test examples translated\n",
    "- ✅ Predictions saved to `outputs/predictions/test_results.json`\n",
    "- ✅ Initial quality check performed\n",
    "\n",
    "**Quick Results**:\n",
    "- Total examples: (calculated during execution)\n",
    "- Idiom accuracy: (calculated during execution)\n",
    "\n",
    "**Next Step**: Run `05_evaluation.ipynb` for comprehensive evaluation with BLEU scores and detailed metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}